<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />


  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2017-09-13"/>
  <meta property="article:created" itemprop="dateCreated" content="2017-09-13"/>
  <meta name="article:author" content="Jonathan Regenstein"/>


  <link rel="stylesheet" type="text/css" href="css/tree.css" />
  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #545454; font-weight: bold; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #a1024a; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007faa; font-weight: bold; } /* ControlFlow */
code span.ch { color: #008000; } /* Char */
code span.cn { color: #d91e18; } /* Constant */
code span.co { color: #545454; } /* Comment */
code span.cv { color: #545454; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #aa5d00; } /* DataType */
code span.dv { color: #a1024a; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #a1024a; } /* Float */
code span.fu { color: #4254a7; } /* Function */
code span.im { } /* Import */
code span.in { color: #545454; font-weight: bold; } /* Information */
code span.kw { color: #007faa; font-weight: bold; } /* Keyword */
code span.op { color: #696969; } /* Operator */
code span.ot { color: #007faa; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #008000; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #008000; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #008000; } /* VerbatimString */
code span.wa { color: #545454; font-weight: bold; font-style: italic; } /* Warning */
</style>

  <!--radix_placeholder_meta_tags-->
  <title>Deep Learning Methods for Urban Feature Extraction</title>


  <style type="text/css">

  body {
    background-color: white;
  }

  .pandoc-table {
    width: 100%;
  }

  .pandoc-table>caption {
    margin-bottom: 10px;
  }

  .pandoc-table th:not([align]) {
    text-align: left;
  }

  .pagedtable-footer {
    font-size: 15px;
  }

  .authors-affiliations .orcid-id {
    width: 16px;
    height:16px;
    margin-left: 4px;
    margin-right: 4px;
    vertical-align: middle;
    padding-bottom: 2px;
  }

  d-title .dt-tags {
    margin-top: 1em;
    grid-column: text;
  }

  .dt-tags .dt-tag {
    text-decoration: none;
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0em 0.4em;
    margin-right: 0.5em;
    margin-bottom: 0.4em;
    font-size: 70%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  d-article table.gt_table td,
  d-article table.gt_table th {
    border-bottom: none;
  }

  .html-widget {
    margin-bottom: 2.0em;
  }

  .l-screen-inset {
    padding-right: 16px;
  }

  .l-screen .caption {
    margin-left: 10px;
  }

  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .shaded-content {
    background: white;
  }

  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }

  .hidden {
    display: none !important;
  }

  d-article {
    padding-top: 2.5rem;
    padding-bottom: 30px;
  }

  d-appendix {
    padding-top: 30px;
  }

  d-article>p>img {
    width: 100%;
  }

  d-article h2 {
    margin: 1rem 0 1.5rem 0;
  }

  d-article h3 {
    margin-top: 1.5rem;
  }

  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }

  /* Tweak code blocks */

  d-article div.sourceCode code,
  d-article pre code {
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  }

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: hidden;
  }

  d-article div.sourceCode {
    background-color: white;
  }

  d-article div.sourceCode pre {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }

  d-article pre {
    font-size: 12px;
    color: black;
    background: none;
    margin-top: 0;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  d-article pre a {
    border-bottom: none;
  }

  d-article pre a:hover {
    border-bottom: none;
    text-decoration: underline;
  }

  @media(min-width: 768px) {

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: visible !important;
  }

  d-article div.sourceCode pre {
    padding-left: 18px;
    font-size: 14px;
  }

  d-article pre {
    font-size: 14px;
  }

  }

  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  /* CSS for d-contents */

  .d-contents {
    grid-column: text;
    color: rgba(0,0,0,0.8);
    font-size: 0.9em;
    padding-bottom: 1em;
    margin-bottom: 1em;
    padding-bottom: 0.5em;
    margin-bottom: 1em;
    padding-left: 0.25em;
    justify-self: start;
  }

  @media(min-width: 1000px) {
    .d-contents {
      height: 0;
      grid-column-start: 1;
      grid-column-end: 4;
      justify-self: center;
      padding-right: 3em;
      padding-left: 2em;
    }
  }

  .d-contents nav h3 {
    font-size: 18px;
    margin-top: 0;
    margin-bottom: 1em;
  }

  .d-contents nav h4 {
    font-size: 10px;
    margin-top: 0;
    margin-bottom: 1em;
  }

  .d-contents li {
    list-style-type: none
  }

  .d-contents nav > ul {
    padding-left: 0;
  }

  .d-contents ul {
    padding-left: 1em
  }

  .d-contents nav ul li {
    margin-top: 0.6em;
    margin-bottom: 0.2em;
  }

  .d-contents nav a {
    font-size: 13px;
    border-bottom: none;
    text-decoration: none;
    color: rgba(0, 0, 0, 0.8);
  }

  .d-contents nav a:hover {
    text-decoration: underline solid rgba(0, 0, 0, 0.6)
  }

  .d-contents nav > ul > li > a {
    font-weight: 600;
  }

  .d-contents nav > ul > li > ul {
    font-weight: inherit;
  }

  .d-contents nav > ul > li > ul > li {
    margin-top: 0.2em;
  }


  .d-contents nav ul {
    margin-top: 0;
    margin-bottom: 0.25em;
  }

  .d-article-with-toc h2:nth-child(2) {
    margin-top: 0;
  }


  /* Figure */

  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }

  .figure img {
    width: 100%;
  }

  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }

  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }

  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }

  /* Citations */

  d-article .citation {
    color: inherit;
    cursor: inherit;
  }

  div.hanging-indent{
    margin-left: 1em; text-indent: -1em;
  }


  /* Tweak 1000px media break to show more text */

  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }

    .grid {
      grid-column-gap: 16px;
    }

    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }

  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }

    .grid {
      grid-column-gap: 32px;
    }
  }


  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */

  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }


  /* Anchor.js */

  .anchorjs-link {
    /*transition: all .25s linear; */
    text-decoration: none;
    border-bottom: none;
  }
  *:hover > .anchorjs-link {
    margin-left: -1.125em !important;
    text-decoration: none;
    border-bottom: none;
  }

  /* Social footer */

  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }

  .disqus-comments {
    margin-right: 30px;
  }

  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }

  #disqus_thread {
    margin-top: 30px;
  }

  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }

  .article-sharing a:hover {
    border-bottom: none;
  }

  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }

  .subscribe p {
    margin-bottom: 0.5em;
  }


  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }


  .sidebar-section.custom {
    font-size: 12px;
    line-height: 1.6em;
  }

  .custom p {
    margin-bottom: 0.5em;
  }


  /* Improve display for browsers without grid (IE/Edge <= 15) */

  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }

  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }

  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }

  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }

  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }

  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }

  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }


  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }

  .downlevel .footnotes ol {
    padding-left: 13px;
  }

  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }

  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }

  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }

  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  </style>

  <script type="application/javascript">

  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }

  // show body when load is complete
  function on_load_complete() {

    // add anchors
    if (window.anchors) {
      window.anchors.options.placement = 'left';
      window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
    }


    // set body to visible
    document.body.style.visibility = 'visible';

    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }

    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }

  function init_distill() {

    init_common();

    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);

    // create d-title
    $('.d-title').changeElementType('d-title');

    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);

    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();

    // move posts container into article
    $('.posts-container').appendTo($('d-article'));

    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');

    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;

    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();

    // move refs into #references-listing
    $('#references-listing').replaceWith($('#refs'));

    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-contents a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });

    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');

    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {

      // capture layout
      var layout = $(this).attr('data-layout');

      // apply layout to markdown level block elements
      var elements = $(this).children().not('div.sourceCode, pre, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });


      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });

    // remove code block used to force  highlighting css
    $('.distill-force-highlighting-css').parent().remove();

    // remove empty line numbers inserted by pandoc when using a
    // custom syntax highlighting theme
    $('code.sourceCode a:empty').remove();

    // load distill framework
    load_distill_framework();

    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {

      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;

      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');

      // article with toc class
      $('.d-contents').parent().addClass('d-article-with-toc');

      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

      // add orcid ids
      $('.authors-affiliations').find('.author').each(function(i, el) {
        var orcid_id = front_matter.authors[i].orcidID;
        if (orcid_id) {
          var a = $('<a></a>');
          a.attr('href', 'https://orcid.org/' + orcid_id);
          var img = $('<img></img>');
          img.addClass('orcid-id');
          img.attr('alt', 'ORCID ID');
          img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
          a.append(img);
          $(this).append(a);
        }
      });

      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }

      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");

      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }

       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }

      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();

      // clear polling timer
      clearInterval(tid);

      // show body now that everything is ready
      on_load_complete();
    }

    var tid = setInterval(distill_post_process, 50);
    distill_post_process();

  }

  function init_downlevel() {

    init_common();

     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));

    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;

    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();

    // remove toc
    $('.d-contents').remove();

    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });


    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);

    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();

    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });

    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));

    $('body').addClass('downlevel');

    on_load_complete();
  }


  function init_common() {

    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};

        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });

        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);

    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});

    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });

      }
    });

    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });

    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }

    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');

    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");

    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();

    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }

  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });

  </script>

  <!--/radix_placeholder_distill-->
  <script src="packages/header-attrs-2.3/header-attrs.js"></script>
  <script src="packages/htmlwidgets-1.5.1/htmlwidgets.js"></script>
  <script src="packages/jquery-1.11.3/jquery.min.js"></script>
  <link href="packages/dygraphs-1.1.1/dygraph.css" rel="stylesheet" />
  <script src="packages/dygraphs-1.1.1/dygraph-combined.js"></script>
  <script src="packages/dygraphs-1.1.1/shapes.js"></script>
  <script src="packages/moment-2.8.4/moment.js"></script>
  <script src="packages/moment-timezone-0.2.5/moment-timezone-with-data.js"></script>
  <script src="packages/moment-fquarter-1.0.0/moment-fquarter.min.js"></script>
  <script src="packages/dygraphs-binding-1.1.1.6/dygraphs.js"></script>
  <script src="packages/anchor-4.2.2/anchor.min.js"></script>
  <script src="packages/bowser-1.9.3/bowser.min.js"></script>
  <script src="packages/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="packages/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>
<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Urban","description":"Feature Extraction","authors":[],"publishedDate":"2021-12-08T00:00:00.000-05:00"}
</script>




<div class="d-title">
<h1>Urban Feature Extraction For Building Performance Studies</h1>
<!--radix_placeholder_categories-->
<div class="dt-tags">
<div class="dt=tag">Author: Nada Tarkhan</div>
<div class="dt=tag">Affiliation: MIT</div>
<div class="dt=tag">Date: 12/09/2021</div>
</div>
<!--/radix_placeholder_categories-->
<p><p>In this post, a brief review and experiments on facade parsing methods for building performance evaluation are presented.</p></p>
</div>


<div class="d-article">
<div class="d-contents">
<nav class="l-text toc figcaption" id="TOC">
<h3>Contents</h3>
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#urban">Urban & Building Analyses: Input Features</a></li>
<li><a href="#CNNs">Extraction Methods</a></li>
<li><a href="#data">Data Set: Acquisition & Labelling</a></li>
<li><a href="#experiments">Experiments</a></li>
<li><a href="#insights">Insights</a></li>
</ul>
</nav>
</div>


  <!--/PART 1:OVERVIEW-->
<h2 id="overview">Overview</h2>
<p>In this post, deep learning methods for Urban feature extraction are presented and discussed. Computer Vision methods have greatly affected the way in which we measure contextual attributes from our urban environments.</p>
<p>Extensive literature has assessed the application of different CNN-based methods to extract facade elements and segment different features.
However, how do these methods translate to building performance studies, specifically with the kinds of assessments that are typically used to evaluate different
building performance phenomena (such as building energy use)? Moreover, what is the kind of data that needs to be collected and at what resolution? The post intends to break this down along with a discussion on data acquisition.
  Finally, a case study will be presented for window extraction.  </p>


  <strong>Challenge 1: Urban Diversity and Scale</strong><br/>
    <p>The Urban environment is very diverse. There is no simple way to standardize the rules needed to interpret different contexts around the world.
    Some of those diversities are represented in the form of the following parameters; building heights, density, urban vegetation, design styles, construction period and building materials.   </p>
  <strong>Challenge 2: Method Matching Needed</strong><br/>
    <p>The challenge here lies in selecting extraction methods that match the task at hand. For instance, if extracting facade areas is required, then applying standard bounding box detection would be
    insufficient. This is expanded upon in the experiments section when an example extraction task is carried out and assessed.</p>

    <strong>Challenge 3: Data Quality</strong><br/>
    <p>As with most machine learning tasks, models are data hungry. As this application represents a more domain specific application, the data quality
    and quantity is imperative to make sound assumptions in simulaiton models. In this application, street view imagery has been the most promising data source,
    yet does not come without challenges and biases. In the Data Acquisition section, the data retrieval and labelling process is discussed.</p>



    <div class="layout-chunk" data-layout="l-page">
 <p> <i><a href="https://www.turing.ac.uk/research/research-programmes/urban-analytics">Image Source</a></i> </p>
 <div class="grid column">
  <div class="row">
    <p><img src="images/urban.jpg" width="624" /></p>

  </div>
 </div>
 </div>



<p>The below blog aims to lay out the following: </p>
    <li>Provide an overview of the building attributes that are required to conduct different types of urban level analyses</li>
    <li>Give a general breakdown of scene segmentation and extraction methods</li>
    <li>Outline a brief history of building and facade parsing literature</li>
    <li>Highlight the main parts of the data acquisition procedures</li>
    <li>Conduct an experiment in window feature extraction from buildings</li>
    <li>Discuss main insights, assessment techniques and results</li>



  <!--/PART 2: URBAN FEATURES-->
  <h2 id="urban">Urban & Building Analyses: Input Features</h2>
  <p>In this section, the main parameters needed for 3 distinct types of building performance studies are outlined. For each study, a set of inputs are required that are
      often very tedious to collect through manual methods. At the same time, in the absence of this data many high level assumptions need to be made
      which can be inaccurate. Hence, automating this data retrieval in the context of urban environments is imperative to the accuracy of these assessment models.
     </p>

  <!-- graph -->

  <div class="layout-chunk" data-layout="l-page">
    <p> <i> Expand the nodes to view the building attributes required for each analysis! </i> </p>


    <div id="tree" style="height: 100%; width: 50%"></div>
  </div>



  <script src="https://cdnjs.cloudflare.com/ajax/libs/d3/4.13.0/d3.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lodash.js/4.17.5/lodash.js"></script>

  <!-- Tree -->


  <h4>Energy</h4>
  <p>This mainly involves whole building/urban energy simulations that looks to quantify annual energy use over the course of a year.
  These simulations take in various building properties as well as the specific urban context and climate files to conduct multi-physics calculations</p>

  <h4>Daylighting</h4>
  <p>This assessment involves simulating building models to evaluate the percentage of occupied time in which a certain daylight threshold is met
  or exceeded. This is highly dependent on window design and placement as well as annual sun exposure.</p>
  <h4>Outdoor Thermal Comfort</h4>
  <p>This assessment factors in thermal properties of an environment to give a measure of thermal satisfaction based on human physiological models.</p>
  <h4>Indoor Thermal Comfort</h4>
  <p>This study aims to achieve the same goal as the above but with internal thermal properties while utilizing indoor thermal comfort thresholds.</p>




  <!--/PART 3:METHODS-->
<h2 id="CNNs">Extraction Methods</h2>
<p>As noted earlier, many computer vision methods have been applied to the area of urban feature extraction. Below are the most notable contributions to this
area, categorized under their respective methods and models:</p>
  <strong>Grammar Based Methods</strong><br/>
    <p>These approaches center on procedural shape grammar  definitions.  At a high level, this method is usually made up of a set of
        hand-crafted rules of basic shapes  which  represents  structured  geometries encoded as parametric sub-divisions.
        These rules can then applied to images to make deductions on hierarchies and the presence of certain elements. Although intuitive to construct,
    hand-crafted features, based on methods such as sobel operators, usually suffer when handling more diverse conditions, especially
        when dealing with noisy street view imagery and architectural styles that fall outside the
    constrained set of pre-defined rules. In addition to this, the extensive computational time does not justify the limited inference capacity. </p>
  <strong>Learning Based Methods</strong><br/>
    <h5>CNNs</h5><br/>
    <p>Recently, Convolutional Neural Networks (CNNs) have dominated image analysis tasks due to their superior performance across classification, object detection,
        semantic segmentation and image segmentation tasks. CNNs have been applied extensively to facade parsing tasks with varying architectures and model
        set-ups. CNNs take in an input image, assign learnable weights and biases to various features in the image and output segmented images based on the detected classes.
        This is mainly carried out through the application of relevant filters. Below, the variations of CNNs applied to facade parsing applications is covered.
    <h5>Polygon RNN</h5><br/>
    <p>Contrasting current object segmentation tasks under CNNs where they are treated as pixel-labeling problems, Polygon RNN casts
        this as a polygon prediction task, matching the annotation style of most annotation datasets.</p>

    <h5>Panoptic Segmentation</h5><br/>
    <p> Panoptic segmentation unifies the typically distinct tasks of semantic segmentation
    (assign a class label to each pixel) and instance segmentation (detect and segment each object instance).</p>

<div class="layout-chunk" data-layout="l-page">
  <p> <i> Hover on the below to view the respective method category and click to be navigated to the research on the method! </i> </p>
      <div class="grid column">
          <div class="row">

            <a href="https://www.sciencedirect.com/science/article/abs/pii/S0360132321005096" target="_blank">
        <figure class="effect-julia">
          <img src="images/pixel.PNG" alt="imgCH"/>
          <figcaption>
            <h5>Pixel Labelling<span>,Rule-Based</span></h5>
            <div>
              <p>Category: Grammar based</p>

            </div>
          </figcaption>
        </figure>
            </a>


              <a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?referer=https://scholar.google.com/&httpsredir=1&article=4851&context=sis_research" target="_blank">
                  <figure class="effect-julia">
                      <img src="images/cnns.PNG" alt="imgSF"/>
                      <figcaption>
                          <h5>CNNs<span></span></h5>
                          <div>
                              <p>Category: Learning Based</p>

                          </div>
                      </figcaption>
                  </figure>
              </a>


            <a href="https://ieeexplore.ieee.org/document/6836030" target="_blank">
        <figure class="effect-julia">
          <img src="images/cnn.png" alt="imgNY"/>
          <figcaption>
            <h5>CNNs<span></span></h5>
            <div>
                <p>Category: Learning Based</p>
            </div>
          </figcaption>
        </figure>
            </a>

            <a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Kirillov_Panoptic_Segmentation_CVPR_2019_paper.html" target="_blank">
        <figure class="effect-julia">
          <img src="images/pan.png" alt="imgSF"/>
          <figcaption>
            <h5>Panoptic Segmentation<span></span></h5>
            <div>
                <p>Category: Learning Based</p>

            </div>
          </figcaption>
        </figure>
            </a>


            <a href="https://www.semanticscholar.org/paper/A-CONVOLUTIONAL-NETWORK-FOR-SEMANTIC-FACADE-AND-Schmitz-Mayer/9c255e49f16390c09832c5f46674cf2b7ef53aa1" target="_blank">
              <figure class="effect-julia">
                <img src="images/cnn3.PNG" alt="imgSF"/>
                <figcaption>
                  <h5>Graph Grammars<span></span></h5>
                  <div>
                      <p>Category: Grammar Based</p>

                  </div>
                </figcaption>
              </figure>
            </a>


            <a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Castrejon_Annotating_Object_Instances_CVPR_2017_paper.html" target="_blank">
              <figure class="effect-julia">
                <img src="images/poly.PNG" alt="imgSF"/>
                <figcaption>
                  <h5>Polygon RNN<span></span></h5>
                  <div>
                      <p>Category: Learning Based</p>

                  </div>
                </figcaption>
              </figure>
            </a>


      </div>

      </div>
</div>


    <br> <p>  </p><br/>


  <strong>A History of Facade Parsing</strong><br/>
  <p>More specific to facade parsing, below is a brief literature review that gives an outlook into the methods utilized and the
  dataset used. I chose to highlight the transfer learning component as this has been shown to improve efficieny as it
      reduces the resources and amount of labelled data required to train new models.</p>


  <div class="layout-chunk" data-layout="l-body">
    <figure class="base-grid" id="optimization-objectives">
      <style>
        #optimization-objectives .objectives {
          grid-column: text;
          grid-template-columns: repeat(1, 1fr);
        }

        #optimization-objectives .objective {
          display: grid;
          grid-template-columns: repeat(2, 1fr) 1.1fr;
        }

        #optimization-objectives .objectives figcaption {
          padding: 4px 8px;
          word-wrap: break-word;
          word-break: break-word;
        }

        #optimization-objectives .objective .objective-icon {
          padding: 8px;
        }


      </style>


      <style>
        #feature-vis-history .row {
        }
        #feature-vis-history .line {
          width: 900px;
        }
        #feature-vis-history .row .info {
          display: inline-block;
          width: 320px;
          height: 56px;
          vertical-align: top;
        }
        #feature-vis-history .row .info img {
          width: 56px;
          height: 56px;
          border-radius: 5px;
          background: #EEE;
        }
        #feature-vis-history figcaption {
          line-height: 16px;
          vertical-align: top;
        }
        #feature-vis-history .row .info figcaption {
          width: 250px;
          margin-left: 8px;
          display: inline-block;
        }
        #feature-vis-history .row.header-row .info figcaption {
          margin-left: 0;
        }
        #feature-vis-history .header-row .sub-headers {
          display: inline-block;
          margin-top: 16px;
          margin-bottom: 10px;
          vertical-align: top;
        }
        #feature-vis-history .header-row .sub-headers figcaption {
          width: 100px; /*104px;*/
          display: inline-block;
          word-wrap: break-word;
          word-break: keep-all;
        }

        #feature-vis-history .row .spacer {
          display: inline-block;
          height: 48px;
          width: 2px;
        }

        #feature-vis-history .info p {
          margin-bottom: 4px;
        }

        #feature-vis-history .info .paper-title {
          font-weight: bold;
        }

        #feature-vis-history .info .paper-text {

        }

        @media (max-width: 1100px) {
          #feature-vis-history .row .spacer {
            display: inline-block;
            height: 48px;
            width: 24px;
          }
          #feature-vis-history .line {
            width: 750px;
          }
        }

        #feature-vis-history .row .category-check-container {
          /*vertical-align: middle;*/
          display: inline-block;
          margin: 8px;
          margin-left: 0px;
          margin-right: 70px;
          width: 32px;
          height: 32px;
          border-radius: 5px;
          border: 1px solid #CCC;
        }
        #feature-vis-history .row .category-check-container .category-check {
          margin: 6px;
          width: 20px;
          height: 20px;
          border-radius: 6px;
        }
        #feature-vis-history .row .category-check-container .set {
          background: #909092;
        }
        #feature-vis-history .line {
          margin-top: 4px;
          margin-bottom: 4px;
          margin-left: 5px;
          height: 1px;
          background: #EAEAEA;
        }

      </style>

      <figure style="grid-column: screen; min-height: 750px;" id="feature-vis-history">
          <div svelte-1206690277="" class="row header-row"><div class="info"></div>
            <div class="spacer"></div>
            <div style="display: inline-block;"><div class="info" style="display: block; width: 200px;"><figcaption style="width: 200px; height: 60px;"><b>Architecture</b>
              Categorization of Main framework</figcaption></div>
              <div class="sub-headers" style="display: block;"><figcaption><b><div>Grammar</div><div style="margin-top: 0px;">-based</div></b></figcaption>
                <figcaption><b><div>CNN</div><div style="margin-top: 0px;"></div></b></figcaption>
                <figcaption><b><div>Mask</div><div style="margin-top: 0px;">R-CNN</div></b></figcaption></div></div>
            <div class="spacer"></div>
            <div style="display: inline-block;"><div class="" style="display: block;"><figcaption style="width: 190px; height: 60px;"><b>Dataset</b>
              Sources</figcaption></div>
              <div class="sub-headers" style="display: block;"><figcaption><b><div>Transfer</div><div style="margin-top: 0px;">Learning</div></b></figcaption>
                <figcaption><b><div>Annotated</div><div style="margin-top: 0px;">Dataset</div></b></figcaption></div></div></div>

          <div svelte-1206690277="" class="row"><div class="info"><img src="images/test.png">
            <figcaption><p class="paper-title">Zhao, <i>et al.</i>, 2010<noscript></noscript> <d-cite key="erhan2009visualizing"></d-cite></p>
              <p class="paper-text">proposed parsing using procedural grammars</p></figcaption></div>

            <div class="spacer"></div>
            <div class="category-check-container"><div class="category-check set"></div></div><div class="category-check-container"><div class="category-check "></div></div><div class="category-check-container"><div class="category-check "></div></div>
            <div class="spacer"></div>
            <div class="category-check-container"><div class="category-check "></div></div><div class="category-check-container"><div class="category-check set "></div></div></div>


          <div svelte-1206690277="" class="line"></div><div svelte-1206690277="" class="row"><div class="info"><img src="images/2.PNG">
          <figcaption><p class="paper-title">Wendel, <i>et al.</i>, 2010<noscript></noscript> <d-cite key="szegedy2013intriguing"></d-cite></p>
            <p class="paper-text">Utilized Scale–Invariant Feature Trans-form (SIFT) </p></figcaption></div>

          <div class="spacer"></div>
          <div class="category-check-container"><div class="category-check set"></div></div><div class="category-check-container"><div class="category-check "></div></div><div class="category-check-container"><div class="category-check "></div></div>
          <div class="spacer"></div>
          <div class="category-check-container"><div class="category-check "></div></div><div class="category-check-container"><div class="category-check set"></div></div></div>

          <div svelte-1206690277="" class="line"></div><div svelte-1206690277="" class="row"><div class="info"><img src="images/3.1.PNG">
          <figcaption><p class="paper-title">Mathias, 2011<noscript></noscript> <d-cite key="mahendran2015understanding"></d-cite></p>
            <p class="paper-text">Utilized inverse procedural modeling</p></figcaption></div>

          <div class="spacer"></div>
          <div class="category-check-container"><div class="category-check set "></div></div><div class="category-check-container"><div class="category-check"></div></div><div class="category-check-container"><div class="category-check "></div></div>
          <div class="spacer"></div>
          <div class="category-check-container"><div class="category-check "></div></div><div class="category-check-container"><div class="category-check set "></div></div></div>


          <div svelte-1206690277="" class="line"></div><div svelte-1206690277="" class="row"><div class="info"><img src="images/4.PNG">
          <figcaption><p class="paper-title">Schmitz, <i>et al.</i>, 2016<noscript></noscript> <d-cite key="nguyen2015deep"></d-cite></p>
            <p class="paper-text">Transfer Learning and small model size</p></figcaption></div>

          <div class="spacer"></div>
          <div class="category-check-container"><div class="category-check "></div></div><div class="category-check-container"><div class="category-check set"></div></div><div class="category-check-container"><div class="category-check "></div></div>
          <div class="spacer"></div>
          <div class="category-check-container"><div class="category-check set"></div></div><div class="category-check-container"><div class="category-check "></div></div></div>


          <div svelte-1206690277="" class="line"></div><div svelte-1206690277="" class="row"><div class="info"><img src="images/5.PNG">
          <figcaption><p class="paper-title">Fathalla, <i>et al.</i>, 2017<noscript></noscript> <d-cite key="mordvintsev2015inceptionism"></d-cite></p>
            <p class="paper-text">Introduced used appearance &layout cues + VGG model.</p></figcaption></div>

          <div class="spacer"></div>
          <div class="category-check-container"><div class="category-check "></div></div><div class="category-check-container"><div class="category-check set"></div></div><div class="category-check-container"><div class="category-check "></div></div>
          <div class="spacer"></div>
          <div class="category-check-container"><div class="category-check "></div></div><div class="category-check-container"><div class="category-check set"></div></div></div>


          <div svelte-1206690277="" class="line"></div><div svelte-1206690277="" class="row"><div class="info"><img src="images/6.PNG">
          <figcaption><p class="paper-title">Koch, <i>et al.</i>, 2018<noscript></noscript> <d-cite key="oygard2015vis"></d-cite></p>
            <p class="paper-text">Utilizes multi-scale patch-based pattern extraction + CNN<br /></p></figcaption></div>

          <div class="spacer"></div>
          <div class="category-check-container"><div class="category-check "></div></div><div class="category-check-container"><div class="category-check set"></div></div><div class="category-check-container"><div class="category-check "></div></div>
          <div class="spacer"></div>
          <div class="category-check-container"><div class="category-check "></div></div><div class="category-check-container"><div class="category-check set"></div></div></div>


          <div svelte-1206690277="" class="line"></div><div svelte-1206690277="" class="row"><div class="info"><img src="images/7.PNG">
          <figcaption><p class="paper-title">, <i>Bacharidis et al.</i>, 2020<noscript></noscript> <d-cite key="tyka2016bilateral"></d-cite></p>
            <p class="paper-text"> Combined generative  adversarial  networks  (GANs)<br /></p></figcaption></div>

          <div class="spacer"></div>
          <div class="category-check-container"><div class="category-check "></div></div><div class="category-check-container"><div class="category-check set"></div></div><div class="category-check-container"><div class="category-check"></div></div>
          <div class="spacer"></div>
          <div class="category-check-container"><div class="category-check "></div></div><div class="category-check-container"><div class="category-check set"></div></div></div>


          <div svelte-1206690277="" class="line"></div><div svelte-1206690277="" class="row"><div class="info"><img src="images/3.PNG">
          <figcaption><p class="paper-title">Hu, <i>et al.</i>, 2020<noscript></noscript> <d-cite key="mordvintsev2016deepdreaming"></d-cite></p>
            <p class="paper-text">Uses bounding boxes detection by using  <br />YOLO architecture in real-time </p></figcaption></div>

          <div class="spacer"></div>
          <div class="category-check-container"><div class="category-check "></div></div><div class="category-check-container"><div class="category-check set"></div></div><div class="category-check-container"><div class="category-check "></div></div>
          <div class="spacer"></div>
          <div class="category-check-container"><div class="category-check "></div></div><div class="category-check-container set"><div class="category-check set"></div></div></div>


          <div svelte-1206690277="" class="line"></div><div svelte-1206690277="" class="row"><div class="info"><img src="images/9.PNG">
          <figcaption><p class="paper-title">Wenguang <i>et al.</i>, 2020<noscript></noscript> <d-cite key="nguyen2016synthesizing"></d-cite></p>
            <p class="paper-text">FasterR-CNN   architecture   for   window   detection.</p></figcaption></div>

          <div class="spacer"></div>
          <div class="category-check-container"><div class="category-check "></div></div><div class="category-check-container"><div class="category-check set "></div></div><div class="category-check-container"><div class="category-check "></div></div>
          <div class="spacer"></div>
          <div class="category-check-container"><div class="category-check set"></div></div><div class="category-check-container"><div class="category-check "></div></div></div>

          <div svelte-1206690277="" class="line"></div><div svelte-1206690277="" class="row"><div class="info"><img src="images/10.PNG">
          <figcaption><p class="paper-title">Nordmark <i>et al.</i>, 2021<noscript></noscript> <d-cite key="nguyen2016plug"></d-cite></p>
            <p class="paper-text">Utilizes Regional Proposal Network</p></figcaption></div>
          <div class="spacer"></div>
          <div class="category-check-container"><div class="category-check "></div></div><div class="category-check-container"><div class="category-check "></div></div><div class="category-check-container"><div class="category-check set "></div></div>
          <div class="spacer"></div>
          <div class="category-check-container"><div class="category-check set"></div></div><div class="category-check-container"><div class="category-check set "></div></div></div>


          <div svelte-1206690277="" class="line"></div><!---->

      </figure>
    </figure>
  </div>



    <strong>Method Matching & Trends</strong><br/>
    <p> The brief history of facade parsing methods shows the following trends:
    <li>A heavy emphasis on CNN networks due to their performance in learning capacity and inference capabilities</li>
    <li>Mask R-CNN provides additional capabilities when it comes to providing a segmentation mask as well for area extraction</li></p>
    <li>Datasets are becoming richer as more facade classes are also making appearances in public datasets (such as balconies, cornices, staircases etc.)</li></p>
    <li>Transfer Learning and data augmentation are becoming more standard practices in the field</li></p>
    <p>In order to select the appropriate method of extraction, the end goal of the assessment must be clear. To
    present a degree of specificity to the subject matter, the below presents the custom case study of extracting window to wall ratios for energy and daylight studies.
    In these studies, the window to wall ratio is a large driver behind thermal envelope performance as well as daylight access. For more granular assessments, deriving
    the exact window area would be needed hence, bounding box methods indicating location would not be sufficient in this case. Similarly, we can argue
    that for applications requiring height estimation, it would be beneficial to have calibrated (actual) building height rather than a ratio/proportion between the different
    object heights in a scene (Trees, buildings etc.)</p>





  <!--/PART 4:DATA-->

  <h2 id="data">Data Set: Acquisition & Labelling</h2>

  <p>Automating the collection of semantic  data  regarding  building  features is required to carry out this task.
      Manual  data acquisition methods can be extremely challenging and time consuming. For  example,  information
      regarding  location,  shapes  and  sizes,  materials  of building  structures  can change over time and hence datasets would need to be both reliable
  and up to date. In this section we discuss the data resolutions, properties and how it is sourced. </p>
  <strong>Data Considerations</strong></p>
    <h5> RELIABILITY </h5></br>

    <p>Street View imagery is often quite noisy and requires heavy preprocessing and rectification. The process of data acquisition starts with automating the extraction of
    frontal facade images. This is done through calculating the camera bearing angle and identifying the centerpoint of the building of interest. Next, a rectification
    procedure requiring homography is applied such that an orthogonal fronto-parallel image is produced. </p>

    <h5><br> AVAILABILITY </h5></br>
    <p>Drawing on the previous point, while several platforms have enabled wide access to street-view data, data can sometimes still be quite sparse in various locations
    around the world. In the case of this example, the data was obtained from Google Street View Imagery. </p>

    <h5><br> QUALITY/QUANTITY </h5></br>
    <p>As the goal is to collect data for building performance studies, we must be presented with a sufficient data set that widely encompasses different building typologies,
    uses, construction periods and neighbourhoods. As with all learning problems, sparcity of data may result in biased deductions and inferences. The below diagram summarizes
    additional data challenges that are specific to urban contexts and buildings.</p>

    <div class="layout-chunk" data-layout="l-page">
        <div class="grid column">
            <div class="row"><img src="images/challenges2.png" alt="imgCH"/></div>
        </div>
    </div>

    <strong>Labelling</strong></p>
    <p>The training data included annotated images that were sourced from the following public facade datasets; TSG-20○2, TSG-60○3, ZuBuD○4, CMP and ECP. To improve model performance, additional labeled data was added. These manual annotations of window geometry were labelled using the <a href="https://github.com/wkentaro/labelme">labelme tool</a>.Retaining the order of corner points was
    imperative to this labelling process- an example on a facade is shown below:</p>

    <div class="layout-chunk" data-layout="l-body">
        <p><img src="images/lab.gif" width="624" /></p>
    </div>


  <p>As we can see, this can be a very time-consuming process that could benefit from automation and publicly avaialable annotated datasets. </p>





<!--/PART 5:EXPERIMENTS-->
<h2 id="experiments">Experiments</h2>
    <strong>Model Description</strong><br/>
    <p>For the sake of experimentation, we utilize a framework developed by Li, Chuan-Kang, et al [1]. The method employs a key-point detection framework that utilizes CNNs
    to make predictions on the presence of window geometry by identifying corners. Resnet is utilized as the backbone network followed by 2 prediction
    branches. One provides heatmaps (for corner points) while the other provides tagmaps (based on grouping of windows and center verification).
    The below table summarizes the model parameters </p>

    <p>The input image is fed into the CNN architecture,
    called the backbone. It is a pre-trained standard convolutional neural network, using ResNet to detect low resolution features.
    The backbone network acts as a  feature extractor over an entire image.

    <p>This workflow along with model parameters is highlighted below.</p>
    <div class="layout-chunk" data-layout="l-page">
        <div class="grid column">
            <div class="row"><img src="images/model2.png" alt="imgCH"/></div>
        </div>
    </div>




    <strong>Training</strong><br/>
    <p>The training process involved a few iterations. Below, the iterations that yielded performance improvements are plotted. The number of training epochs against the loss
    are displayed along with the specific iteration changes. The iterations mainly involved different data augmentation methods, more annotated data as inputs and longer training
    durations. We see a steady decline in loss values as we increase training time and feed the model more data, which is as expected. </p>
  <p> <i> Click on the value headings to toggle plots. </i> </p>   
<div class="layout-chunk" data-layout="l-body">
        <!-- training plot -->
        <div id="my_dataviz"></div>
        <img src="images/test.png" alt="imgCH"/>

    </div>


    <strong>Quantitative Scoring</strong><br/> </p>
    <p>Following the model training procedures, this experiment involved segmenting the data into different batches of assessment and then measuring performance on these batches.
        The batches of images were organized into three categories; Design variations, Environmental variations and Contextual Variations. These are explained further in the table below.</p>

    <table>
        <tr>
            <th><STRONG>DESIGN</STRONG></th>
            <td>Description: Includes design variations including conditions
                where model showed low prediction performance. This includes; small windows and dense facades, fully glazed facades, large buildings,
                special conditions such as window insets, heavy facade textures etc. </td>

        </tr>
        <tr>
            <td><strong>ENVIRONMENT</strong></td>
            <td>Description: Includes environmental variations such as lighting conditions, shadow occlusions and low resolution images from street view imagery. </td>

        </tr>
        <tr>
            <td><strong>CONTEXT</strong></td>
            <td>Description: Even though diversity in architectural styles is a core goal, not all design styles can be captured. The training data images
                have facades from all around the world however since we want to conduct analysis in specific locations, data from New York and Lisbon were a bigger focus.</td>

        </tr>



    </table>

    <p><img src="images/results3.png" width="624" /></p>
    <p><strong>IOU Metric:</strong>The Intersection over Union (IoU) metric is a method to quantify the percent overlap between the target mask and our prediction output. A higher IOU indicates better prediction performance</p>
    <p><strong>Precision (over 50% IoU):</strong> The amount of positive predictions that were correct.Calculation: true positives / number of predicted positives.
    </p>
    <p><strong>Recall (over 50% IoU):</strong> The percentage of positive cases that were caught. Calculation: true positives / number of actual positives</p>

    <p><strong>Results:</strong>The results we see here compare the above scores from 2 models; iteration 1 and iteration 4 (as highlighted under the training section).
        We see significant performance across all three metrics especially when looking at the IoU scores. </p>



  <strong>Qualitative Scoring</strong><br/> </p>
  <p>How then can we evaluate the CNN method performance against more traditional non-learning based methods? To answer this question a comparison between two methods is carried out; Method 1 uses grammar based sobel operators for edge detection as carried
      out by Szcześniak et al [2]. And Method 2 utilizes CNNs as highlighted under model description. Below is
  a checklist of evaluation criteria that captures how effective the method is at sufficiently evaluating important attributes. </p>
  <table>
    <tr>
      <th><h3>Criteria</h3></th>
      <th><h3>Method 1: Pixel Counting</h3></th>
      <th><h3>Method 2: Keypoint Detection using CNNs</h3></th>
    </tr>
    <tr>
      <td><strong>Non-uniform window alignment</strong></td>
      <td>Windows must be on uniform gridding</td>
      <td>Achieved ✓</td>
    </tr>
    <tr>
      <td><strong>Non-uniform Window geometry</strong></td>
      <td>Angled non-rectangular forms not detected</td>
      <td>Arched forms detected as orthogonal</td>
    </tr>
    <tr>
      <td><strong>Multi-material facades</strong></td>
      <td>Misclassification as windows can occur</td>
      <td>Achieved ✓</td>
    </tr>
    <tr>
      <td><strong>Textures and patterns</strong></td>
      <td>Texture lines may be mis-classified as edges</td>
      <td>Achieved ✓</td>
    </tr>
    <tr>
      <td><strong>Lighting conditions and shadow occlusions</strong></td>
      <td>Achieved ✓</td>
      <td>Achieved ✓</td>
    </tr>
    <tr>
      <td><strong>Fully Glazed Facades</strong></td>
      <td>Achieved ✓</td>
      <td>Additional training data was required to improve detection in this condition</td>
    </tr>
    <tr>
      <td><strong>Façade obstruction elements (balconies, staircases)</strong></td>
      <td>Mis-classification as windows can occur</td>
      <td>Achieved ✓</td>
    </tr>
    <tr>
      <td><strong>Densely populated facades</strong></td>
      <td>Achieved ✓</td>
      <td>Additional training data was required to improve detection in this condition</td>
    </tr>
    <tr>
      <td><strong>Depth perception (inset windows)</strong></td>
      <td>This may sometimes not be detected by algorithm</td>
      <td>Achieved ✓</td>
    </tr>

  </table>

        <strong>Visual Inspection & Results</strong><br/> </p>
        <p>Below is a set of facades of different conditions and diversities. The Green box annotation captures the detected window extents.
        The grammar based method represents the utilization of sobel operators. As summarized in the table above, we see the pixel based method failing in more complex
        facade designs, where other facade elements are mistaken for windows. </p>

    <p><strong>Method Comparison Results</strong></p>

            <div class="row">    <p><img src="images/compare.png" width="624" /></p></div>


    <p><strong>Results showing detection in Facade Diversity</strong></p>

                <div class="row">    <p><img src="images/cover.png" width="624" /></p></div>


    <p><strong>Results showing CNN- Model Improvements- From Iteration 1 (Old Model) to Iteration 4 (Improved Model)</strong></p>

            <div class="row">    <p><img src="images/res.png" width="624" /></p></div>



    <h2 id="insights">Insights</h2><br/> </p>
    <p>The format of this blog intends to reflect on the methods while carrying out an experiment in a grammar based method versus a CNN based method.
        The format intends to reflect on the outputs and the relevance to the building performance field. The below citations and linked studies provide
        information on more specific model architectures and parameters.

        Based on the above literature review and experiments, below are some discussion points for method enhancements and expanding this work:</p>
    <li>Extracting window areas in urban contexts provides key insight into energy, daylighting and thermal comfort studies</li>
    <li>The tested CNN method provides enhanced performance over rigid rule-based frameworks yet also displays some weaknesses. Circular window geometries
    cannot be detected. Methods such as panoptic segmentation could add this level of granularity to detection</li>
    <li>Data is becoming more accessible in urban environments, yet noise and low image quality issues may still persist. The selection of datasets must be given close attention as this is
        a large determinant of model and prediction performance.
        <li>The largest model performance boost came from feeding in additional labelled data and training for longer periods of time.
    The experiments section provides an example of testing images against categorical batches to make sure model improvements are going in the right direction.</li>
    <li>As this area of work is expanding rapidly and making their way into new domains, more rigorous evaluation system must be put in place.</li>








</div>

</div>





<!--APPENDIX-->

<div class="d-appendix">


</div>


<div class="appendix-bottom">
<h3 id="updates-and-corrections">Citations</h3>
  <p>Methods used for Experimentation:</p>
  <p>[1] TLi, Chuan-Kang, et al. "Window Detection in Facades Using Heatmap Fusion." Journal of Computer Science and Technology 35.4 (2020): 900-912.</p>
  <p>[2] Szcześniak, Jakub T., et al. "A method for using street view imagery to auto-extract window-to-wall ratios and its relevance for urban-level daylighting
      and energy simulations." Building and Environment (2021): 108108.</p>
    <p>Other references:</p>
  <p>Zhao, Peng et al. (2010). “Rectilinear parsing of architecture in urban environment”. In: 2010 IEEE
      Computer Society Conference on Computer Vision and Pattern Recognition. IEEE, pp. 342–349</p>
    <p>Wendel, Andreas, Michael Donoser, and Horst
        Bischof (2010). “Unsupervised facade segmentation using repetitive patterns”. In: Joint Pattern
        Recognition Symposium. Springer, pp. 51–60.</p>
    <p>Mathias, Markus et al. (2011). “Automatic architectural style recognition”. In: ISPRS-International
        Archives of the Photogrammetry, Remote Sensing
        and Spatial Information Sciences 3816, pp. 171–
        176.</p>
    <p>Schmitz, Matthias and Helmut Mayer (2016). “A
        convolutional network for semantic facade segmentation and interpretation”. In: The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences 41, p. 709.</p>

 <p></p>
<p>Koch, David et al. (2018). “Visual estimation of
    building condition with patch-level ConvNets”.
    In: Proceedings of the 2018 ACM Workshop on
    Multimedia for Real Estate Tech, pp. 12–17.</p>
    <p>Bacharidis, Konstantinos, Froso Sarri, and Lemonia
        Ragia (2020). “3D building façade reconstruction
        using deep learning”. In: ISPRS International
        Journal of Geo-Information 9(5), p. 322.</p>
    <p>Hu, Han et al. (2020). “Fast and Regularized Reconstruction of Building Fa\c {c} ades from
        Street-View Images using Binary Integer Programming”. In: arXiv preprint arXiv:2002.08549</p>
    <p>Ma, Wenguang and Wei Ma (2020). “Deep window
        detection in street scenes”. In: KSII Transactions
        on Internet and Information Systems (TIIS) 14(2),
        pp. 855–870.</p>
    <p>Castrejon, Lluis, et al. "Annotating object instances with a polygon-rnn." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.</p>
    <p>Kirillov, Alexander, et al. "Panoptic segmentation." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.</p>
<p>Liu, Hantang, et al. "Deepfacade: A deep learning approach to facade parsing." (2017).</p>
    <p>Koziński, Mateusz, and Renaud Marlet. "Image parsing with graph grammars and Markov Random Fields applied to facade analysis." IEEE Winter Conference on Applications of Computer Vision. IEEE, 2014.</p>

    <p>The following is the source<a href="https://github.com/Nadatarkhan/6.S898_FinalProject">repository</a>.</p>
</div>





</body>

</html>

<script defer src="js/gif-slider.js"></script>
<script src="js/training.js"></script>
<link rel="stylesheet" type="text/css" href="css/styles.css">
<script src="https://kit.fontawesome.com/a076d05399.js"></script>
<script src="./js/tree.js"></script>
<script src="https://d3js.org/d3-scale-chromatic.v1.min.js"></script>
<link rel="stylesheet" type="text/css" href="css/set2.css" />


